from pyspark.sql import SparkSession
from pyspark import SparkContext

if __name__ == "__main__":
    spark = SparkSession.builder.appName("python").master("local").getOrCreate()
    input_file_path = "file:///C://Users//sashirekha.bisati//Desktop//data.txt"

    tech_rdd = spark.sparkContext.textFile(input_file_path)
    counts = tech_rdd.flatMap(lambda line: line.split(" ")) \
        .map(lambda word: (word, 1)) \
        .reduceByKey(lambda a, b: a + b)
    add_counts = counts.collect()
    for element in add_counts:
        print(element)
