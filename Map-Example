package com.spark.scala

import org.apache.spark.sql.{Column, Row, SparkSession}
import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
import org.apache.spark.sql.functions._

import scala.collection.mutable
object Map_Example {
  def main(args: Array[String]):Unit = {
    val spark = SparkSession.builder().master("local").appName("Map_Example").getOrCreate()
    val structureData = Seq(
      Row("36636","Finance",Row(3000,"USA")),
      Row("40288","Finance",Row(5000,"IND")),
      Row("42114","Sales",Row(3900,"USA")),
      Row("39192","Marketing",Row(2500,"CAN")),
      Row("34534","Sales",Row(6500,"USA"))
    )

    val structureSchema = new StructType()
      .add("id",StringType)
      .add("dept",StringType)
      .add("properties",new StructType()
        .add("salary",IntegerType)
        .add("location",StringType)
      )

    var df = spark.createDataFrame(
      spark.sparkContext.parallelize(structureData),structureSchema)
    df.printSchema()
    df.show(false)
    val index = df.schema.fieldIndex("properties")
    val propSchema = df.schema(index).dataType.asInstanceOf[StructType]
    var columns = mutable.LinkedHashSet[Column]()
    propSchema.fields.foreach(field =>{
      columns.add(lit(field.name))
      columns.add(col("properties." + field.name))
    })

    df = df.withColumn("propertiesMap",map(columns.toSeq:_*))
    df = df.drop("properties")
    df.printSchema()
    df.show(false)
    df.select(col("id"),map_keys(col("propertiesMap"))).show(false)
  }
  }

