package com.spark.scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
case class Log(page: String, visitor: String)

object Sample_count_Distinct {
  def main(args: Array[String]):Unit = {
    val spark = SparkSession.builder().master("local").appName("test").getOrCreate()
    import spark.implicits._

   val info = spark.sparkContext
      .textFile("C://Users//sashirekha.bisati//Desktop//Page_visitor.txt")
      .map(_.split(","))
      .map(attributes => Log(attributes(0), attributes(1))).toDF()

    val result = info.select("page","visitor")
      .groupBy('page)
      .agg('page, countDistinct('visitor))
    result.show()

//group by date
     val l  = List( (12,56,"ab","1-06-2020"),(123,58,"gh","2-06-2020"),(44,66,"ty","1-06-2020"))

    val df = spark.sparkContext.parallelize(l).toDF("id","price","tag","date")

    val grpaBY = df.groupBy("tag","date").agg(avg("price")).orderBy("date")

        grpaBY.show(10)





  }

}
